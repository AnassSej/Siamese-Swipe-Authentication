{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the compressed file\n",
    "loaded_Data = np.load('Final.npz',allow_pickle=True)\n",
    "\n",
    "# create a new DataFrame from the loaded data\n",
    "colData=pd.DataFrame(loaded_Data)\n",
    "Data = pd.DataFrame({colData[0][col]: loaded_Data[colData[0][col]] for col in range(0, len(colData))})\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple for choosing number of samples\n",
    "samples=50\n",
    "df=pd.DataFrame((Data['Device'].value_counts()>=samples).value_counts())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese architecture: shared feature extractor\n",
    "def create_base_network(input_dim):\n",
    "    input = Input(shape=(input_dim,))\n",
    "    x = Dense(128, activation='relu')(input)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "# Distance function\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "# Prepare pairs of data\n",
    "def create_pairs(data, labels):\n",
    "    pairs, targets = [], []\n",
    "    label_dict = {label: data[labels == label] for label in np.unique(labels)}\n",
    "    for label in label_dict:\n",
    "        examples = label_dict[label]\n",
    "        for i in range(len(examples) - 1):\n",
    "            pairs += [[examples[i], examples[i + 1]]]\n",
    "            targets += [1]\n",
    "            neg_label = np.random.choice([l for l in label_dict if l != label])\n",
    "            neg_example = label_dict[neg_label][np.random.randint(len(label_dict[neg_label]))]\n",
    "            pairs += [[examples[i], neg_example]]\n",
    "            targets += [0]\n",
    "    return np.array(pairs), np.array(targets)\n",
    "minn=50\n",
    "while minn>0.2 :\n",
    "    # Efface la sortie précédente\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    array_EER = []\n",
    "    device_list = pd.DataFrame(Data['Device'].unique())\n",
    "    device_data = {}\n",
    "    Train = pd.DataFrame(columns=Data.columns)\n",
    "    Test = pd.DataFrame(columns=Data.columns)\n",
    "    \n",
    "    for i in range(len(device_list)):\n",
    "        dev_id = device_list.iloc[i, 0]\n",
    "        if (Data['Device'] == dev_id).sum() >= samples:\n",
    "            df = Data[Data['Device'] == dev_id].head(samples)\n",
    "            split_idx = int(samples * 0.9)\n",
    "            dfTrain, dfTest = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "            Train = pd.concat([Train, dfTrain], ignore_index=True)\n",
    "            Test = pd.concat([Test, dfTest], ignore_index=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_all = scaler.fit_transform(Train.drop('Device', axis=1).values)\n",
    "    y_train_all = Train['Device'].values\n",
    "    \n",
    "    # Create Siamese network\n",
    "    input_dim = X_train_all.shape[1]\n",
    "    base_network = create_base_network(input_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    distance = Lambda(euclidean_distance)([processed_a, processed_b])\n",
    "    siamese_net = Model([input_a, input_b], distance)\n",
    "    siamese_net.compile(loss=contrastive_loss, optimizer=Adam(1e-3))\n",
    "    \n",
    "    # Generate training pairs\n",
    "    pairs, targets = create_pairs(X_train_all, y_train_all)\n",
    "    \n",
    "    # Train the Siamese network\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='loss', patience=5, verbose=1),\n",
    "        ModelCheckpoint('New_best_siamese_CLEA50.keras', save_best_only=True, monitor='loss', verbose=1)\n",
    "    ]\n",
    "    siamese_net.fit([pairs[:, 0], pairs[:, 1]], targets, batch_size=64, epochs=20, callbacks=callbacks)\n",
    "    \n",
    "    # EER computation per device\n",
    "    devices = Train['Device'].unique()\n",
    "\n",
    "    Rep=0\n",
    "    Count=0\n",
    "    for device in devices:\n",
    "        X_test = scaler.transform(Test[Test['Device'] == device].drop('Device', axis=1).values)\n",
    "        X_imposter = scaler.transform(Test[Test['Device'] != device].drop('Device', axis=1).values)\n",
    "        X_train = scaler.transform(Train[Train['Device'] == device].drop('Device', axis=1).values)\n",
    "        ref = X_train[:1]\n",
    "    \n",
    "        dist_genuine = siamese_net.predict([np.repeat(ref, len(X_test), axis=0), X_test]).flatten()\n",
    "        dist_imposter = siamese_net.predict([np.repeat(ref, len(X_imposter), axis=0), X_imposter]).flatten()\n",
    "\n",
    "        # Sweep through a range of threshold values\n",
    "        mn = min(dist_genuine)\n",
    "        mx = max(dist_genuine)\n",
    "        thresholds = np.linspace(mn, mx, 100)  # Use np.linspace to avoid creating too large an array\n",
    "        cl = 30\n",
    "        distance = len(dist_genuine) + len(dist_imposter)\n",
    "        for threshold in thresholds:\n",
    "            # Create a binary array indicating whether each data point is an anomaly or not\n",
    "            y_predI = np.where(dist_imposter > threshold, 0, 1)\n",
    "            y_predO = np.where(dist_genuine > threshold, 0, 1)\n",
    "            impo = 0\n",
    "            while impo < len(dist_imposter):\n",
    "                confidence = 60\n",
    "                for j in range(impo, impo + len(dist_genuine)):\n",
    "                    if confidence > 0 and confidence > cl:\n",
    "                        if y_predI[j] == 0:\n",
    "                            confidence -= 10\n",
    "                        else:\n",
    "                            if confidence < 100:\n",
    "                                confidence += 5\n",
    "                    else:\n",
    "                        y_predI[j] = 0\n",
    "                impo += len(dist_genuine)\n",
    "            confidence = 60\n",
    "            for j in range(len(dist_genuine)):\n",
    "                if confidence > 0 and confidence > cl:\n",
    "                    if y_predO[j] == 0:\n",
    "                        confidence -= 10\n",
    "                    else:\n",
    "                        if confidence < 100:\n",
    "                            confidence += 5\n",
    "                else:\n",
    "                    y_predO[j] = 0\n",
    "                    \n",
    "        y_true = np.concatenate([y_predO, y_predI])\n",
    "        y_scores = np.concatenate([dist_genuine, dist_imposter])\n",
    "\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            print(\"Warning: y_true has only one class, skipping EER computation.\")\n",
    "            eer=0.5\n",
    "        else:\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, -y_scores)\n",
    "            fnr = 1 - tpr\n",
    "            eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
    "            eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "        \n",
    "        array_EER.append(round(eer * 100, 2))\n",
    "        print(round(eer * 100, 2))\n",
    "        eer_percent = round(eer * 100, 2)\n",
    "        Rep=Rep+1\n",
    "        if eer_percent==50.0:\n",
    "            Count=Count+1\n",
    "\n",
    "        # Sauvegarde temporaire\n",
    "        device_data[device] = {\n",
    "            \"eer\": eer_percent,\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"X_imposter\": X_imposter\n",
    "        }\n",
    "        \n",
    "        # ceci sort de la boucle for, mais pas du while\n",
    "        if Rep==5 and Count==5:\n",
    "            print(\"Break !!!\")\n",
    "            break\n",
    "    \n",
    "\n",
    "    # Sélection du meilleur device (min EER)\n",
    "    best_device = min(device_data, key=lambda d: device_data[d][\"eer\"])\n",
    "    best = device_data[best_device]\n",
    "    \n",
    "    # Sauvegarde dans .npz\n",
    "    np.savez(\"New_best_device_data_CLEA50.npz\",\n",
    "             X_train=best[\"X_train\"],\n",
    "             X_test=best[\"X_test\"],\n",
    "             X_imposter=best[\"X_imposter\"],\n",
    "             device=best_device)\n",
    "    \n",
    "    # Summary\n",
    "    print('EERs for all devices:', array_EER)\n",
    "    print('Min EER:', min(array_EER))\n",
    "    print('Max EER:', max(array_EER))\n",
    "    minn=min(array_EER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
